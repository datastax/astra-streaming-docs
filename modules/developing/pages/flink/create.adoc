= Create a Flink job in Astra Streaming

[NOTE]
====
This guide is part of a series for building streaming applications with {product_name} and Apache Flink. The other guides in the series are:

* xref:flink/index.adoc[]
* xref:flink/connect.adoc[]
* xref:flink/manage.adoc[]

You will need your flink endpoint address within Astra Streaming. If you don't have this, please contact the team at streaming@datastax.com or open a support ticket asking for this info.
====

A Flink *job* is a collection of one or more Flink applications that are submitted to a Flink cluster for execution. In a Flink job, data is typically ingested from various sources such as Kafka, HDFS, or databases. The data is then processed using Flink's high-level APIs or low-level operators, which provide a wide range of transformations and operations for data manipulation, filtering, aggregation, and analytics.

Overall, a Flink job enables you to process and analyze large-scale data sets efficiently, with low-latency and high throughput, making it suitable for various use cases such as real-time analytics, event-driven applications, data pipelines, and ETL (Extract, Transform, Load) processes.

It is important that you have pre-created both the input and output topics in Pulsar before attempting to create a Flink job. Both topics must have a schema that is compatible with the Flink table schema. In the example below we will be using a simple JSON schema.

== Pre-requisites
For the following steps you will need these environment variables:

[source,bash]
----
ASTRA_TOKEN=AstraCS:XXXX
TENANT_NAME=<YOUR_ASTRA_STREAMING_TENANT_NAME>
FLINK_INPUT_TOPIC_NAME=flink-input-names
FLINK_OUTPUT_TOPIC_NAME=flink-names-output
----

== Create the input topic

. Create a new topic that will be the starting point of your Fink job
+
[tabs]
====
Pulsar Admin::
+
--
[source, bash]
----
./bin/pulsar-admin topics create \
  "persistent://$TENANT_NAME/flink/$FLINK_INPUT_TOPIC_NAME"
----
--

Result::
+
--
success
--
====

. Create a new schema definition named 'input-names-schema.json'
+
[source, json]
----
{
  "type": "JSON",
  "schema": "{\"type\": \"record\",\"namespace\":\"com.example\",\"name\":\"Person\",\"fields\":[{\"name\":\"personName\",\"type\":\"string\"}]}",
  "properties": {}
}
----

. Upload the schema definition to the topic
+
[tabs]
====
Pulsar Admin::
+
--
[source, bash]
----
./bin/pulsar-admin schemas upload \
  --filename input-names-schema.json \
  "persistent://$TENANT_NAME/flink/$FLINK_INPUT_TOPIC_NAME"
----
--

Result::
+
--
success
--
====

== Create the output topic

. Create a new topic that will get the output of your Flink job:
+
[tabs]
====
Pulsar Admin::
+
--
[source, bash]
----
./bin/pulsar-admin topics create \
  "persistent://$TENANT_NAME/flink/$FLINK_OUTPUT_TOPIC_NAME"
----
--

Result::
+
--
success
--
====

. Create a new schema definition named 'output-names-schema.json'
+
[source, json]
----
{
  "type": "JSON",
  "schema": "{\"type\": \"record\",\"namespace\":\"com.example\",\"name\":\"Person\",\"fields\":[{\"name\":\"personName\",\"type\":\"string\"}]}",
  "properties": {}
}
----

. Upload the schema definition to the topic
+
[tabs]
====
Pulsar Admin::
+
--
[source, bash]
----
./bin/pulsar-admin schemas upload \
  --filename output-names-schema.json \
  "persistent://$TENANT_NAME/flink/$FLINK_OUTPUT_TOPIC_NAME"
----
--

Result::
+
--
success
--
====

== Validate topic configuration

To ensure your topics are configured correctly, run the following commands and observe the successful output.

[tabs]
====
Pulsar Admin::
+
--
[source, bash]
----
./bin/pulsar-admin schemas get \
  "persistent://$TENANT_NAME/flink/$FLINK_INPUT_TOPIC_NAME"

./bin/pulsar-admin schemas get \
  "persistent://$TENANT_NAME/flink/$FLINK_OUTPUT_TOPIC_NAME"
----
--

Input Topic Result::
+
--
[source,json]
----
{
  "version": 0,
  "schemaInfo": {
    "name": "persons",
    "schema": {
      "type": "record",
      "namespace": "com.example",
      "name": "Person",
      "fields": [
        {
          "name": "name",
          "type": "string"
        }
      ]
    },
    "type": "JSON",
    "timestamp": 1684964118894,
    "properties": {}
  }
}
----
--

Output Topic Result::
+
--
[source,json]
----
{
  "version": 0,
  "schemaInfo": {
    "name": "persons-output",
    "schema": {
      "type": "record",
      "namespace": "com.example",
      "name": "Person",
      "fields": [
        {
          "name": "personName",
          "type": "string"
        }
      ]
    },
    "type": "JSON",
    "timestamp": 1685120472299,
    "properties": {}
  }
}
----
--

====

You should get a JSON output of the schema definition just created. This confirms the topics are ready to be a part of your flink job.

== Create the Flink job

. Create the job definition file and name it 'persons-flink-job.json'. Don't forget to replace the tenant name with your tenant name and the input topic name with the topic name set in the environment variables.
+
[source, json]
----
{
  "parallelism": 1,
  "outputTable": "person-names-output",
  "createTableSpecs": [
    {
     "tableName": "namesInput",
     "format": "json",
     "topic": "<YOUR_ASTRA_STREAMING_TENANT_NAME>/flink/<FLINK_INPUT_TOPIC_NAME>",
     "columns": ["personName string"]
    }
  ],
  "selectQuery": "select input.personName from flink.namesInput as input"
}
----

. Upload the definition and activate the job. Don't forget to replace the flink endpoint address with your assigned value and the tenant name with your tenant name.
+
[tabs]
====
Curl::
+
--
[source, bash]
----
curl -H "Content-Type: application/json" \
  -H "Authorization: Bearer $ASTRA_TOKEN" \
  -X POST \
  -d @persons-flink-job.json \
  https://<FLINK_ENDPOINT_ADDRESS>/flink/api/jobs/$TENANT_NAME
----
--

Result::
+
--
[source, json]
----
{
  "specs": {
    "name": "j-flink-tenant-vwnrwv",
    "outputTable": "person-names-output",
    "selectQuery": "select input.personName from flink.namesInput as input",
    "createTableSpecs": [
      {
        "tableName": "namesInput",
        "columns": ["personName string"],
        "topic": ".../flink/$FLINK_INPUT_TOPIC_NAME",
        "format": "json"
      }
    ],
    "parallelism": 1
  },
  "status": {
    "lifecycleState": "CREATED"
  }
}
----
--
====

Nice! You've created a Flink job! Next we'll check its status, view its logs, and manage it in xref:flink/manage.adoc[].
