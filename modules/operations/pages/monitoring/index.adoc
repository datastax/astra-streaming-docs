= Monitor streaming tenants
:navtitle: Monitoring overview

Because {product} is a managed SaaS offering, some https://pulsar.apache.org/docs/reference-metrics/[{pulsar-reg} metrics] aren't exposed for external integration purposes.
At a high level, {product} only exposes metrics related to namespaces.
Metrics that are not directly related to namespaces aren't exposed externally, such as the Bookkeeper ledger and journal metrics and Zookeeper metrics.

Additionally, of the exposed metrics, not all metrics are recommended for external integration.

== {pulsar-short} raw metrics

For a complete {pulsar-short} metrics reference, see:

* https://pulsar.apache.org/docs/reference-metrics/#namespace-metrics[Namespace metrics]

* https://pulsar.apache.org/docs/reference-metrics/#topic-metrics[Topic metrics]

For a complete {product} metrics reference, see xref:monitoring/metrics.adoc[].

== {product} metrics

=== Namespace and topic metrics

{product} exposes both namespace and topic level metrics.
Namespace metrics can always be inferred from corresponding topic metrics via metrics aggregation.

The following table lists recommended namespace and/or topic metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$namespace-topic-metrics.csv[]
|===

=== Replication metrics

When geo-replication is enabled for a particular namespace, a subset of namespace metrics is available specifically for geo-replication purposes. Below is the list of recommended geo-replication metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$replication-metrics.csv[]
|===

=== Subscription metrics

The following table gives the list of recommended subscription metrics as a starting point.

[%header,format=csv,cols="2,1,3"]
|===
include::example$subscription-metrics.csv[]
|===

=== Function metrics

The following table gives the list of recommended function metrics as a starting point. This is only relevant when {pulsar-short} functions are deployed in {product}.

[%header,format=csv,cols="2,1,3"]
|===
include::example$function-metrics.csv[]
|===

=== Source connector metrics

The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when {pulsar-short} source connectors are deployed in {product}.

[%header,format=csv,cols="2,1,3"]
|===
include::example$source-connector-metrics.csv[]
|===

=== Sink connector metrics

The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when {pulsar-short} sink connectors are deployed in {product}.
[%header,format=csv,cols="2,1,3"]
|===
include::example$sink-connector-metrics.csv[]
|===

[#aggregate-astra-streaming-metrics]
== Aggregate {product} metrics

[IMPORTANT]
====
Do _not_ aggregate metrics on shared clusters because one cluster can be shared among multiple organizations.
For more information, see xref:astream-limits.adoc[] and xref:operations:astream-pricing.adoc[].
====

Each externally exposed raw {product} metric is reported at a very low level, at each individual server instance (the `exported_instance` label) and each topic partition (the `topic` label). The same raw metrics could come from multiple server instances. From a {product} user's perspective, the direct monitoring of raw metrics is not really useful. Raw metrics need to be aggregated first - for example, by averaging or summing the raw metrics over a period of time.

The following example shows some raw metrics for a {pulsar-short} message backlog (`pulsar_msg_backlog`) scraped from an {product} cluster in the Google Cloud `us-central1` region:

[source,console]
----
....
pulsar_msg_backlog{app="pulsar", cluster="pulsar-gcp-uscentral1", component="broker", controller_revision_hash="pulsar-gcp-uscentral1-broker-<hash>f", exported_instance="<ip>:<port>", exported_job="broker", helm_release_name="astraproduction-gcp-pulsar-uscentral1", instance="prometheus-gcp-uscentral1.streaming.datastax.com:443", job="astra-pulsar-metrics-demo", kubernetes_namespace="pulsar", kubernetes_pod_name="pulsar-gcp-uscentral1-broker-3", namespace="demo/testns", prometheus="pulsar/astraproduction-gcp-pulsar-prometheus", prometheus_replica="prometheus-astraproduction-gcp-pulsar-prometheus-0", pulsar_cluster_dns="gcp-uscentral1.streaming.datastax.com", release="astraproduction-gcp-pulsar-uscentral1", statefulset_kubernetes_io_pod_name="pulsar-gcp-uscentral1-broker-3", topic="persistent://demo/testns/raw-partition-0"}
....
----

To transform raw metrics into a usable state, {company} recommends the following:

* Aggregate metrics at the parent topic level, at minimum, instead of at the partition level.
In {pulsar-short}, end user applications only deal with messages at the parent topic level; however, internally, {pulsar-short} handles message processing at the partition level.

* Exclude reported metrics that are associated with {product}'s system namespaces and topics, which are usually prefixed by two underscores, such as:
+
[source,plain]
----
__kafka
__transaction_producer_state
----

=== PromQL query patterns

PromQL is Prometheus's simple and powerful query language that you can use to select and aggregate time series data in real time.
For more information, see the https://prometheus.io/docs/prometheus/latest/querying/basics/[PromQL documentation].

{company} recommends the following PromQL query patterns for aggregating raw {product} metrics.
The following examples use the `pulsar_msg_backlog` raw metric to demonstrate the patterns.
In accordance with the recommendations in <<aggregate-astra-streaming-metrics>>, the example patterns aggregate messages at the parent topic level or higher and they exclude system topics.

.Filter system topics
[%collapsible]
====
You can use the following expression to filter system topics:

[source,pgsql]
----
{topic !~ ".*__.*"}`
----

This expression excludes messages with topic labels that include two consecutive underscores.
This works because {pulsar-short} system topics and namespaces are usually prefixed by two underscores, such as:

[source,plain]
----
persistent://some_tenant/__kafka/__consumer_offsets_partition_0
----

To use this expression, your applications' namespace and topic names don't contain double underscores.
If they do, they will also be excluded by this filter.
====

==== Get the total message backlog of a specific parent topic, excluding system topics

`$ptopic` is a Grafana dashboard variable that represents a specific parent topic.

[source,pgsql]
----
sum(pulsar_msg_backlog{topic=~$ptopic, topic !~ ".*__.*"})
----

==== Get the total message backlog of a specific namespace, excluding system topics

`$namespace` is a Grafana dashboard variable that represents a specific namespace.

[source,pgsql]
----
sum(pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

==== Get the total message backlog of a tenant, excluding system topics

`$tenant` is a (Grafana dashboard) variable that represents a specific tenant.

[source,pgsql]
----
sum(pulsar_msg_backlog{namespace=~"$tenant.+"", topic !~ ".*__.*"})
----

==== Get the total message backlog of each topic within a specific namespace, excluding system topics

[source,pgsql]
----
sum by(topic) (pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

==== Get the top 10 message backlog by topic within a specific namespace, excluding system topics

[source,pssql]
----
topk (10, sum by(topic) (pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"}))
----

== Metrics alerts

Most of the exposed {product} metrics reflect generic application workload characteristics, such as message rate or throughput, and they are for informational purposes only.

However, {company} recommends that you monitor the following metrics for unexpected increases:

.Metrics for alerting
[%header,format=csv,cols="2,2,1,3"]
|===
include::example$alert-metrics.csv[]
|===

=== Alerting rules

In a perfect world, these metrics would always be `0`.
In reality, these metrics will increase when an application's workload increases, and then return to normal when the workload decreases.

You can set an alert threshold to be notified when these metrics exceed normal capacity, but this can cause false alarms during expected workload spikes.

Alternatively, you can calculate the metrics' increase rate over a period of time, such as one hour, and then set a threshold based on the rate of increase.
For example, if the average message backlog increase rate exceeds the given threshold, an alert is triggered.

Thresholds for these metrics depends on your application's routine workloads and requirements.
Generally, these values are large positive numbers, ranging in the several hundreds or several thousands.
If your receive too many false alarms, adjust the alert threshold to a higher value.

== See also

* xref:monitoring/metrics.adoc[]
* xref:monitoring/integration.adoc[]
* xref:monitoring/new-relic.adoc[]