= Monitor streaming tenants
:navtitle: Monitoring overview

Because {product} is a software-as-a-service product, not all Apache Pulsar metrics (https://pulsar.apache.org/docs/reference-metrics/[Pulsar Metrics Reference]) are exposed for external integration purposes. At a high level, {product} only exposes metrics that are related to namespaces. Other metrics that are not directly namespace related are not exposed externally, such as the Bookkeeper ledger and journal metrics and Zookeeper metrics.

In the following sections, we'll explore each of the {product} metrics categories that are available for external integration, and recommended metrics for external integration.

== Pulsar raw metrics

For a complete Pulsar metrics reference, see:

* https://pulsar.apache.org/docs/reference-metrics/#namespace-metrics[Namespace metrics]

* https://pulsar.apache.org/docs/reference-metrics/#topic-metrics[Topic metrics]

For a complete {product} metrics reference, see xref:monitoring/metrics.adoc[].

== {product} metrics

=== Namespace and topic metrics

{product} exposes both namespace and topic level metrics.
Namespace metrics can always be inferred from corresponding topic metrics via metrics aggregation.

The following table lists recommended namespace and/or topic metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$namespace-topic-metrics.csv[]
|===

=== Replication metrics

When geo-replication is enabled for a particular namespace, a subset of namespace metrics is available specifically for geo-replication purposes. Below is the list of recommended geo-replication metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$replication-metrics.csv[]
|===

=== Subscription metrics

The following table gives the list of recommended subscription metrics as a starting point.

[%header,format=csv,cols="2,1,3"]
|===
include::example$subscription-metrics.csv[]
|===

=== Function metrics

The following table gives the list of recommended function metrics as a starting point. This is only relevant when Pulsar functions are deployed in {product}.

[%header,format=csv,cols="2,1,3"]
|===
include::example$function-metrics.csv[]
|===

=== Source connector metrics

The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when Pulsar source connectors are deployed in {product}.

[%header,format=csv,cols="2,1,3"]
|===
include::example$source-connector-metrics.csv[]
|===

=== Sink connector metrics

The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when Pulsar sink connectors are deployed in {product}.
[%header,format=csv,cols="2,1,3"]
|===
include::example$sink-connector-metrics.csv[]
|===

[#aggregate-astra-streaming-metrics]
== Aggregate {product} metrics

[IMPORTANT]
====
Do _not_ aggregate metrics on shared clusters because one cluster can be shared among multiple organizations.
For more information, see xref:astream-limits.adoc[] and xref:operations:astream-pricing.adoc[].
====

Each externally exposed raw {product} metric is reported at a very low level, at each individual server instance (the `exported_instance` label) and each topic partition (the `topic` label). The same raw metrics could come from multiple server instances. From a {product} user's perspective, the direct monitoring of raw metrics is not really useful. Raw metrics need to be aggregated first - for example, by averaging or summing the raw metrics over a period of time.

The following example shows some raw metrics for a Pulsar message backlog (`pulsar_msg_backlog`) scraped from an {product} cluster in the Google Cloud `us-central1` region:

[source,console]
----
....
pulsar_msg_backlog{app="pulsar", cluster="pulsar-gcp-uscentral1", component="broker", controller_revision_hash="pulsar-gcp-uscentral1-broker-<hash>f", exported_instance="<ip>:<port>", exported_job="broker", helm_release_name="astraproduction-gcp-pulsar-uscentral1", instance="prometheus-gcp-uscentral1.streaming.datastax.com:443", job="astra-pulsar-metrics-demo", kubernetes_namespace="pulsar", kubernetes_pod_name="pulsar-gcp-uscentral1-broker-3", namespace="demo/testns", prometheus="pulsar/astraproduction-gcp-pulsar-prometheus", prometheus_replica="prometheus-astraproduction-gcp-pulsar-prometheus-0", pulsar_cluster_dns="gcp-uscentral1.streaming.datastax.com", release="astraproduction-gcp-pulsar-uscentral1", statefulset_kubernetes_io_pod_name="pulsar-gcp-uscentral1-broker-3", topic="persistent://demo/testns/raw-partition-0"}
....
----

To transform raw metrics into a usable state, {company} recommends the following:

* Aggregate metrics at the parent topic level, at minimum, instead of at the partition level.
In Pulsar, end user applications only deal with messages at the parent topic level; however, internally, Pulsar handles message processing at the partition level.

* Exclude reported metrics that are associated with {product}'s system namespaces and topics, which are usually prefixed by two underscores.
For example, enabling Pulsar's Kafka protocol handler through S4K integration, creates a system namespace called `\__kafka` that has one system topic called `\__transaction_producer_state`.

=== PromQL query patterns

PromQL is Prometheus's simple and powerful query language that you can use to select and aggregate time series data in real time.
For more information, see the https://prometheus.io/docs/prometheus/latest/querying/basics/[PromQL documentation].

{company} recommends the following PromQL query patterns for aggregating raw {product} metrics.
These examples use the `pulsar_msg_backlog` raw metric to demonstrate the patterns.
Additionally, in accordance with the recommendations in <<aggregate-astra-streaming-metrics>>, these patterns aggregate messages at the parent topic level or higher and they exclude system topics (with the PromQL statement `{topic !~ ".\*__.*"}`).

.About the system topics filter
[%collapsible]
====
The PromQL pattern `{topic !~ ".\*_\_.*"}` filters out messages with topic labels that do not include two consecutive underscores.
This works because Pulsar system topics and namespaces are usually prefixed by two underscores, such as `persistent://**TENANT_NAME**/\__kafka/\__consumer_offsets_partition_0`.
However, this pattern assumes that your applications' namespace and topic names don't contain double underscores.
If they do, they are also filtered.
====

==== Get the total message backlog of a specific parent topic, excluding system topics

`$ptopic` is a Grafana dashboard variable that represents a specific parent topic.

[source,pgsql]
----
sum(pulsar_msg_backlog{topic=~$ptopic, topic !~ ".*__.*"})
----

==== Get the total message backlog of a specific namespace, excluding system topics

`$namespace` is a Grafana dashboard variable that represents a specific namespace.

[source,pgsql]
----
sum(pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

==== Get the total message backlog of a tenant, excluding system topics

`$tenant` is a (Grafana dashboard) variable that represents a specific tenant.

[source,pgsql]
----
sum(pulsar_msg_backlog{namespace=~"$tenant.+"", topic !~ ".*__.*"})
----

==== Get the total message backlog of each topic within a specific namespace, excluding system topics

[source,pgsql]
----
sum by(topic) (pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

==== Get the top 10 message backlog by topic within a specific namespace, excluding system topics

[source,pssql]
----
topk (10, sum by(topic) (pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"}))
----

== Metrics alerts

Most of the exposed {product} metrics reflect generic application workload characteristics, such as message rate or throughput, and they are for informational purposes only.

However, {company} recommends that you monitor the following metrics for unexpected increases:

.Metrics for alerting
[%header,format=csv,cols="2,2,1,3"]
|===
include::example$alert-metrics.csv[]
|===

=== Alerting rules

In a perfect world, these metrics would always be `0`.
In reality, these metrics will increase when an application's workload increases, and then return to normal when the workload decreases.

You can set an alert threshold to be notified when these metrics exceed normal capacity, but this can cause false alarms during expected workload spikes.

Alternatively, you can calculate the metrics' increase rate over a period of time, such as one hour, and then set a threshold based on the rate of increase.
For example, if the average message backlog increase rate exceeds the given threshold, an alert is triggered.

Thresholds for these metrics depends on your application's routine workloads and requirements.
Generally, these values are large positive numbers, ranging in the several hundreds or several thousands.
If your receive too many false alarms, adjust the alert threshold to a higher value.

== See also

* xref:monitoring/metrics.adoc[]
* xref:monitoring/integration.adoc[]
* xref:monitoring/new-relic.adoc[]

