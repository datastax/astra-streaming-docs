= Monitoring streaming tenants
:navtitle: Monitoring overview

Because {product} is a software-as-a-service product, not all Apache Pulsar metrics (https://pulsar.apache.org/docs/reference-metrics/[Pulsar Metrics Reference]) are exposed for external integration purposes. At a high level, {product} only exposes metrics that are related to namespaces. Other metrics that are not directly namespace related are not exposed externally, such as the Bookkeeper ledger and journal metrics and Zookeeper metrics.

In the following sections, we'll explore each of the {product} metrics categories that are available for external integration, and recommended metrics for external integration.

== Pulsar raw metrics

For a complete Pulsar metrics reference, see:

* https://pulsar.apache.org/docs/reference-metrics/#namespace-metrics[Namespace metrics]

* https://pulsar.apache.org/docs/reference-metrics/#topic-metrics[Topic metrics]

For a complete {product} metrics reference, see xref:monitoring/metrics.adoc[].

== {product} metrics

=== Namespace and topic metrics

{product} exposes both namespace and topic level metrics.
Namespace metrics can always be inferred from corresponding topic metrics via metrics aggregation.

The following table lists recommended namespace and/or topic metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$namespace-topic-metrics.csv[]
|===

=== Replication Metrics

When geo-replication is enabled for a particular namespace, a subset of namespace metrics is available specifically for geo-replication purposes. Below is the list of recommended geo-replication metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$replication-metrics.csv[]
|===

=== Subscription metrics

The following table gives the list of recommended subscription metrics as a starting point.

[%header,format=csv,cols="2,1,3"]
|===
include::example$subscription-metrics.csv[]
|===

=== Function Metrics

The following table gives the list of recommended function metrics as a starting point. This is only relevant when Pulsar functions are deployed in {product}.

[%header,format=csv,cols="2,1,3"]
|===
include::example$function-metrics.csv[]
|===

=== Source connector metrics

The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when Pulsar source connectors are deployed in {product}.

[%header,format=csv,cols="2,1,3"]
|===
include::example$source-connector-metrics.csv[]
|===

=== Sink connector metrics

The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when Pulsar sink connectors are deployed in {product}.
[%header,format=csv,cols="2,1,3"]
|===
include::example$sink-connector-metrics.csv[]
|===

== Aggregate {product} Metrics

Each externally exposed raw {product} metric is reported at a very low level, at each individual server instance (the `exported_instance` label) and each topic partition (the `topic` label). The same raw metrics could come from multiple server instances. From a {product} user's perspective, the direct monitoring of raw metrics is not really useful. Raw metrics need to be aggregated first - for example, by averaging or summing the raw metrics over a period of time.

Below is an example of a raw metric for the Pulsar message backlog (pulsar_msg_backlog) scraped from an {product} cluster located in the GCP US Central region:

.Show raw metric for Pulsar message backlog:
[%collapsible]
====
....
pulsar_msg_backlog{app="pulsar", cluster="pulsar-gcp-uscentral1", component="broker", controller_revision_hash="pulsar-gcp-uscentral1-broker-<hash>f", exported_instance="<ip>:<port>", exported_job="broker", helm_release_name="astraproduction-gcp-pulsar-uscentral1", instance="prometheus-gcp-uscentral1.streaming.datastax.com:443", job="astra-pulsar-metrics-msgenrich", kubernetes_namespace="pulsar", kubernetes_pod_name="pulsar-gcp-uscentral1-broker-3", namespace="msgenrich/testns", prometheus="pulsar/astraproduction-gcp-pulsar-prometheus", prometheus_replica="prometheus-astraproduction-gcp-pulsar-prometheus-0", pulsar_cluster_dns="gcp-uscentral1.streaming.datastax.com", release="astraproduction-gcp-pulsar-uscentral1", statefulset_kubernetes_io_pod_name="pulsar-gcp-uscentral1-broker-3", topic="persistent://msgenrich/testns/raw-partition-0"}
....
====

{company} recommends the following guidelines for aggregating raw metrics:

* Aggregate metrics at the parent topic level, at minimum, instead of at the partition level.
In Pulsar, end user applications only deal with messages at the parent topic level; however, internally, Pulsar handles message processing at the partition level.

* Exclude reported metrics that are associated with {product}'s system namespaces and topics, which are usually prefixed by two underscores (++__++).
For example, enabling Pulsar's Kafka protocol handler through S4K integration, creates a system namespace called ++__kafka++ that has one system topic called ++__transaction_producer_state++.

* Do _not_ aggregate metrics if you are on the *Pay As You Go* {product} plan because one cluster can be shared among multiple organizations.
For more information, see xref:astream-limits.adoc[].

=== PromQL query patterns

Prometheus provides a powerful but easy-to-use query language called PromQL for selecting and aggregating time series data in real time. PromQL syntax is beyond this document's scope, but the https://prometheus.io/docs/prometheus/latest/querying/basics/[Prometheus documentation] is a great place to start.

In the rest of this section, we'll recommend some PromQL query patterns for aggregating raw {product} metrics.
These examples use one {product} raw metric, pulsar_msg_backlog, as an example for illustrative purposes.
We aggregate messages at the parent topic level or above, and exclude system topics per our recommendations above.
We filter out system messages with the pattern +{topic !~ ".*__.*"}+.
This PromQL pattern filters out messages when their topic labels do not include +__+.
This works because Pulsar system topics usually have ++__++ as the topic or namespace name prefix (e.g. persistent://<tenant>/++__++kafka/__consumer_offsets_partition_0).
This pattern assumes that the user applications don't also have namespaces and topics with +__+ as part of the names, or they will be filtered as well.

Pattern 1: Get the total message backlog of a specific parent topic, excluding system topics.
"$ptopic" is a (Grafana dashboard) variable that represents a specific parent topic.
[source,psql]
----
sum(pulsar_msg_backlog{topic=~$ptopic, topic !~ ".*__.*"})
----

Pattern 2: Get the total message backlog of a specific namespace, excluding system topics.
"$namespace" is a (Grafana dashboard) variable that represents a specific namespace.
[source,psql]
----
sum(pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

Pattern 3: Get the total message backlog of a tenant, excluding system topics.
"$tenant" is a (Grafana dashboard) variable that represents a specific tenant.
[source,psql]
----
sum(pulsar_msg_backlog{namespace=~"$tenant.+"", topic !~ ".*__.*"})
----

Pattern 4: Get the total message backlog of each topic within a specific namespace, excluding system topics.
[source,psql]
----
sum by(topic) (pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

Pattern 5: Get the top 10 message backlog by topic within a specific namespace, excluding system topics.
[source,psql]
----
topk by(topic) (10, sum(pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

== Metrics to be alerted

Most of the exposed {product} metrics are for informational purposes only and in most cases the metrics values are just reflecting the application workload characteristics. For example, message rate or throughput are common examples of such metrics.

There are, however, several metrics that need special attention when we see an increasing number of their values. Among the exposed {product} metrics, these metrics are as follows:

.Metrics for alerting
[%header,format=csv,cols="2,2,1,3"]
|===
include::example$alert-metrics.csv[]
|===

=== Alerting rules
In a perfect world, these metrics should always stay at 0, but in reality, these metrics will increase when the application workload becomes heavier.
If your system is behaving correctly, these metrics should go down when the application workload drops.

A simple way to trigger an alert on these metrics is to set a threshold which triggers an alert when the metric exceeds it. However, this will probably cause false alarms during workload spikes.

A better approach is calculating the metrics' increase rate over a period of time (e.g. 1 hour) and setting a threshold on the rate of increase.
For example, if the average message backlog increase rate exceeds a threshold, an alert is triggered.

The actual threshold values for these metrics is highly dependent on each application's workload and requirements, but the values should be relatively large positive numbers, e.g. several hundreds or several thousands.
Otherwise, they may trigger too many false alarms.

== See also

* xref:monitoring/metrics.adoc[]
* xref:monitoring/integration.adoc[]
* xref:monitoring/new-relic.adoc[]

