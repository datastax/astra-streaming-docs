= Monitoring Streaming Tenants
:navtitle: Monitoring overview

Because Astra Streaming is a software-as-a-service product, not all Apache Pulsar metrics (https://pulsar.apache.org/docs/reference-metrics/[Pulsar Metrics Reference]) are exposed for external integration purposes. At a high level, Astra Streaming only exposes metrics that are related to namespaces. Other metrics that are not directly namespace related are not exposed externally, such as the Bookkeeper ledger and journal metrics and Zookeeper metrics.

In the following sections, we'll explore each of the Astra Streaming metrics categories that are available for external integration, and recommended metrics for external integration.

== Pulsar raw metrics

For a complete Pulsar metrics reference, see:

* https://pulsar.apache.org/docs/reference-metrics/#namespace-metrics[Namespace metrics]

* https://pulsar.apache.org/docs/reference-metrics/#topic-metrics[Topic metrics]

For a complete Astra Streaming metrics reference, see xref:monitoring/metrics.adoc[].

== {product_name} metrics

=== Namespace and topic metrics
Astra Streaming exposes both namespace and topic level metrics.
Namespace metrics can always be inferred from corresponding topic metrics via metrics aggregation.

The following table lists recommended namespace and/or topic metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$namespace-topic-metrics.csv[]
|===

=== Replication Metrics
When geo-replication is enabled for a particular namespace, a subset of namespace metrics is available specifically for geo-replication purposes. Below is the list of recommended geo-replication metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$replication-metrics.csv[]
|===

=== Subscription metrics
The following table gives the list of recommended subscription metrics as a starting point.

[%header,format=csv,cols="2,1,3"]
|===
include::example$subscription-metrics.csv[]
|===

=== Function Metrics

The following table gives the list of recommended function metrics as a starting point. This is only relevant when Pulsar functions are deployed in Astra Streaming.

[%header,format=csv,cols="2,1,3"]
|===
include::example$function-metrics.csv[]
|===

=== Source connector metrics
The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when Pulsar source connectors are deployed in Astra Streaming.

[%header,format=csv,cols="2,1,3"]
|===
include::example$source-connector-metrics.csv[]
|===

=== Sink connector metrics
The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when Pulsar sink connectors are deployed in Astra Streaming.
[%header,format=csv,cols="2,1,3"]
|===
include::example$sink-connector-metrics.csv[]
|===

== Aggregate Astra Streaming Metrics

Each externally exposed raw Astra Streaming metric is reported at a very low level, at each individual server instance (the `exported_instance` label) and each topic partition (the `topic` label). The same raw metrics could come from multiple server instances. From a {product_name} user's perspective, the direct monitoring of raw metrics is not really useful. Raw metrics need to be aggregated first - for example, by averaging or summing the raw metrics over a period of time.

Below is an example of a raw metric for the Pulsar message backlog (pulsar_msg_backlog) scraped from an Astra Streaming cluster located in the GCP US Central region:

.Show raw metric for Pulsar message backlog:
[%collapsible]
====
....
pulsar_msg_backlog{app="pulsar", cluster="pulsar-gcp-uscentral1", component="broker", controller_revision_hash="pulsar-gcp-uscentral1-broker-<hash>f", exported_instance="<ip>:<port>", exported_job="broker", helm_release_name="astraproduction-gcp-pulsar-uscentral1", instance="prometheus-gcp-uscentral1.streaming.datastax.com:443", job="astra-pulsar-metrics-msgenrich", kubernetes_namespace="pulsar", kubernetes_pod_name="pulsar-gcp-uscentral1-broker-3", namespace="msgenrich/testns", prometheus="pulsar/astraproduction-gcp-pulsar-prometheus", prometheus_replica="prometheus-astraproduction-gcp-pulsar-prometheus-0", pulsar_cluster_dns="gcp-uscentral1.streaming.datastax.com", release="astraproduction-gcp-pulsar-uscentral1", statefulset_kubernetes_io_pod_name="pulsar-gcp-uscentral1-broker-3", topic="persistent://msgenrich/testns/raw-partition-0"}
....
====

To make raw metrics like this useful for end users, we recommend the following guidelines when aggregating raw metrics:

. Aggregate metrics to at least the parent topic level, instead of at the partition level. In Pulsar, end user applications only deal with messages at the parent topic level (but internally, Pulsar is handling message processing at the partition level).
. Exclude reported metrics that are associated with Astra Streaming's system namespaces and topics. These namespaces and topics normally have a name starting with ++__++ (two underscores). For example, when Pulsar's Kafka protocol handler is enabled (via S4K integration), a system namespace ++__kafka++ is created with one system topic within called ++__transaction_producer_state++.
Do NOT aggregate metrics with the Astra Streaming *Pay As You Go* option, since one cluster may be shared among multiple organizations. For more, see xref:astream-limits.adoc[Cluster Limits].

=== PromQL query patterns

Prometheus provides a powerful but easy-to-use query language called PromQL for selecting and aggregating time series data in real time. PromQL syntax is beyond this document's scope, but the https://prometheus.io/docs/prometheus/latest/querying/basics/[Prometheus documentation] is a great place to start.

In the rest of this section, we'll recommend some PromQL query patterns for aggregating raw Astra Streaming metrics.
These examples use one Astra Streaming raw metric, pulsar_msg_backlog, as an example for illustrative purposes.
We aggregate messages at the parent topic level or above, and exclude system topics per our recommendations above.
We filter out system messages with the pattern +{topic !~ ".*__.*"}+.
This PromQL pattern filters out messages when their topic labels do not include +__+.
This works because Pulsar system topics usually have ++__++ as the topic or namespace name prefix (e.g. persistent://<tenant>/++__++kafka/__consumer_offsets_partition_0).
This pattern assumes that the user applications don't also have namespaces and topics with +__+ as part of the names, or they will be filtered as well.

Pattern 1: Get the total message backlog of a specific parent topic, excluding system topics.
"$ptopic" is a (Grafana dashboard) variable that represents a specific parent topic.
[source,psql]
----
sum(pulsar_msg_backlog{topic=~$ptopic, topic !~ ".*__.*"})
----

Pattern 2: Get the total message backlog of a specific namespace, excluding system topics.
"$namespace" is a (Grafana dashboard) variable that represents a specific namespace.
[source,psql]
----
sum(pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

Pattern 3: Get the total message backlog of a tenant, excluding system topics.
"$tenant" is a (Grafana dashboard) variable that represents a specific tenant.
[source,psql]
----
sum(pulsar_msg_backlog{namespace=~"$tenant.+"", topic !~ ".*__.*"})
----

Pattern 4: Get the total message backlog of each topic within a specific namespace, excluding system topics.
[source,psql]
----
sum by(topic) (pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

Pattern 5: Get the top 10 message backlog by topic within a specific namespace, excluding system topics.
[source,psql]
----
topk by(topic) (10, sum(pulsar_msg_backlog{namespace=~"$namespace", topic !~ ".*__.*"})
----

== Metrics to be alerted
Most of the exposed Astra Streaming metrics are for informational purposes only and in most cases the metrics values are just reflecting the application workload characteristics. For example, message rate or throughput are common examples of such metrics.

There are, however, several metrics that need special attention when we see an increasing number of their values. Among the exposed Astra Streaming metrics, these metrics are as follows:

.Metrics for alerting
[%header,format=csv,cols="2,2,1,3"]
|===
include::example$alert-metrics.csv[]
|===

=== Alerting rules
In a perfect world, these metrics should always stay at 0, but in reality, these metrics will increase when the application workload becomes heavier.
If your system is behaving correctly, these metrics should go down when the application workload drops.

A simple way to trigger an alert on these metrics is to set a threshold which triggers an alert when the metric exceeds it. However, this will probably cause false alarms during workload spikes.

A better approach is calculating the metrics' increase rate over a period of time (e.g. 1 hour) and setting a threshold on the rate of increase.
For example, if the average message backlog increase rate exceeds a threshold, an alert is triggered.

The actual threshold values for these metrics is highly dependent on each application's workload and requirements, but the values should be relatively large positive numbers, e.g. several hundreds or several thousands.
Otherwise, they may trigger too many false alarms.

== See also

* xref:monitoring/metrics.adoc[]
* xref:monitoring/integration.adoc[]
* xref:monitoring/new-relic.adoc[]

