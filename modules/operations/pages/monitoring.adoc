= Externally Exposed Astra Streaming Metrics for Integration

Because Astra Streaming is a software-as-a-service product, not all Apache Pulsar metrics (https://pulsar.apache.org/docs/reference-metrics/[Pulsar Metrics Reference]) are exposed for external integration purposes. At a high level, Astra Streaming only exposes metrics that are related to namespaces. Other metrics that are not directly namespace related are not exposed externally, such as the Bookkeeper ledger and journal metrics and Zookeeper metrics.

In the following sections, we’ll explore each of the Astra Streaming metrics categories that are available for external integration, and recommended metrics for external integration.

== Pulsar raw metrics

If you're just looking for a complete reference for Pulsar metrics, they are available below.

Namespace metrics:

https://pulsar.apache.org/docs/reference-metrics/#namespace-metrics

Topic metrics:

https://pulsar.apache.org/docs/reference-metrics/#topic-metrics

== {product_name} metrics

=== Namespace and topic metrics
Astra Streaming exposes both namespace and topic level metrics.
Namespace metrics can always be inferred from corresponding topic metrics via metrics aggregation.

The following table lists recommended namespace and/or topic metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$namespace-topic-metrics.csv[]
|===

=== Replication Metrics
When geo-replication is enabled for a particular namespace, a subset of namespace metrics is available specifically for geo-replication purposes. Below is the list of recommended geo-replication metrics as a starting point.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$replication-metrics.csv[]
|===

=== Subscription metrics
The following table gives the list of recommended subscription metrics as a starting point.

[%header,format=csv,cols="2,1,3"]
|===
include::example$subscription-metrics.csv[]
|===

=== Function Metrics

The following table gives the list of recommended function metrics as a starting point. This is only relevant when Pulsar functions are deployed in Astra Streaming.

[%header,format=csv,cols="2,1,3"]
|===
include::example$function-metrics.csv[]
|===

=== Source connector metrics
The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when Pulsar source connectors are deployed in Astra Streaming.

[%header,format=csv,cols="2,1,3"]
|===
include::example$source-connector-metrics.csv[]
|===

=== Sink connector metrics
The following table gives the list of recommended source connector metrics as a starting point. This is only relevant when Pulsar sink connectors are deployed in Astra Streaming.
[%header,format=csv,cols="2,1,3"]
|===
include::example$sink-connector-metrics.csv[]
|===

== Aggregate Astra Streaming Metrics

Each externally exposed raw Astra Streaming metric is reported at a very low level, at each individual server instance (the `exported_instance` label) and each topic partition (the `topic` label). The same raw metrics could come from multiple server instances. From a {product_name} user’s perspective, the direct monitoring of raw metrics is not really useful. Raw metrics need to be aggregated first - for example, by averaging or summing the raw metrics over a period of time.

Below is an example of a raw metric for the Pulsar message backlog (pulsar_msg_backlog) scraped from an Astra Streaming cluster located in the GCP US Central region:

.Show raw metric for Pulsar message backlog:
[%collapsible]
====
....
pulsar_msg_backlog{app="pulsar", cluster="pulsar-gcp-uscentral1", component="broker", controller_revision_hash="pulsar-gcp-uscentral1-broker-<hash>f", exported_instance="<ip>:<port>", exported_job="broker", helm_release_name="astraproduction-gcp-pulsar-uscentral1", instance="prometheus-gcp-uscentral1.streaming.datastax.com:443", job="astra-pulsar-metrics-msgenrich", kubernetes_namespace="pulsar", kubernetes_pod_name="pulsar-gcp-uscentral1-broker-3", namespace="msgenrich/testns", prometheus="pulsar/astraproduction-gcp-pulsar-prometheus", prometheus_replica="prometheus-astraproduction-gcp-pulsar-prometheus-0", pulsar_cluster_dns="gcp-uscentral1.streaming.datastax.com", release="astraproduction-gcp-pulsar-uscentral1", statefulset_kubernetes_io_pod_name="pulsar-gcp-uscentral1-broker-3", topic="persistent://msgenrich/testns/raw-partition-0"}
....
====

To make raw metrics like this useful for end users, we recommend the following guidelines when aggregating raw metrics:

. Aggregate metrics to at least the parent topic level, instead of at the partition level. In Pulsar, end user applications only deal with messages at the parent topic level (but internally, Pulsar is handling message processing at the partition level).
. Exclude reported metrics that are associated with Astra Streaming’s system namespaces and topics. These namespaces and topics normally have a name starting with ++__++ (two underscores). For example, when Pulsar’s Kafka protocol handler is enabled (via S4K integration), a system namespace ++__kafka++ is created with one system topic within called ++__transaction_producer_state++.
Do NOT aggregate metrics with the Astra Streaming PAYG option, since one cluster may be shared among multiple organizations. For more, see xref:astream-limits.adoc[Cluster Limits].

=== PromQL query patterns

Prometheus provides a powerful but easy-to-use query language called PromQL for selecting and aggregating time series data in real time. PromQL syntax is beyond this document's scope, but the https://prometheus.io/docs/prometheus/latest/querying/basics/[Prometheus documentation] is a great place to start.

In the rest of this section, we’ll recommend some PromQL query patterns for aggregating raw Astra Streaming metrics.
These examples use one Astra Streaming raw metric, pulsar_msg_backlog, as an example for illustrative purposes.
We aggregate messages at the parent topic level or above, and exclude system topics per our recommendations above.
We filter out system messages with the pattern +{topic !~ ".*__.*"}+.
This PromQL pattern filters out messages when their topic labels do not include +__+.
This works because Pulsar system topics usually have ++__++ as the topic or namespace name prefix (e.g. persistent://<tenant>/++__++kafka/__consumer_offsets_partition_0).
This pattern assumes that the user applications don’t also have namespaces and topics with +__+ as part of the names, or they will be filtered as well.

Pattern 1: Get the total message backlog of a specific parent topic, excluding system topics.
"$ptopic” is a (Grafana dashboard) variable that represents a specific parent topic.
[source,psql]
----
sum(pulsar_msg_backlog{topic=~$ptopic, topic !~ ".*__.*"})
----

Pattern 2: Get the total message backlog of a specific namespace, excluding system topics.
“$namespace” is a (Grafana dashboard) variable that represents a specific namespace.
[source,psql]
----
sum(pulsar_msg_backlog{namespace=~”$namespace”, topic !~ ".*__.*"})
----

Pattern 3: Get the total message backlog of a tenant, excluding system topics.
"$tenant” is a (Grafana dashboard) variable that represents a specific tenant.
[source,psql]
----
sum(pulsar_msg_backlog{namespace=~”$tenant.+”, topic !~ ".*__.*"})
----

Pattern 4: Get the total message backlog of each topic within a specific namespace, excluding system topics.
[source,psql]
----
sum by(topic) (pulsar_msg_backlog{namespace=~”$namespace”, topic !~ ".*__.*"})
----

Pattern 5: Get the top 10 message backlog by topic within a specific namespace, excluding system topics.
[source,psql]
----
topk by(topic) (10, sum(pulsar_msg_backlog{namespace=~”$namespace”, topic !~ ".*__.*"})
----

== Metrics to be alerted
Most of the exposed Astra Streaming metrics are for informational purposes only and in most cases the metrics values are just reflecting the application workload characteristics. For example, message rate or throughput are common examples of such metrics.

There are, however, several metrics that need special attention when we see an increasing number of their values. Among the exposed Astra Streaming metrics, these metrics are:
.Metrics for alerting
[%header,format=csv,cols="2,2,1,3"]
|===
include::example$alert-metrics.csv[]
|===

=== Alerting rules
In a perfect world, these metrics should always stay at 0, but in reality, these metrics will increase when the application workload becomes heavier.
If your system is behaving correctly, these metrics should go down when the application workload drops.

A simple way to trigger an alert on these metrics is to set a threshold which triggers an alert when the metric exceeds it. However, this will probably cause false alarms during workload spikes.

A better approach is calculating the metrics' increase rate over a period of time (e.g. 1 hour) and setting a threshold on the rate of increase.
For example, if the average message backlog increase rate exceeds a threshold, an alert is triggered.

The actual threshold values for these metrics is highly dependent on each application’s workload and requirements, but the values should be relatively large positive numbers, e.g. several hundreds or several thousands.
Otherwise, they may trigger too many false alarms.

== Grafana dashboards for Astra Streaming metrics

DataStax has built Grafana dashboards around core message processing for the exposed Astra Streaming metrics.
The dashboards can be found https://github.com/datastax/astra-streaming-examples/tree/master/grafana-dashboards[on GitHub^]{external-link-icon}.

== Overview Dashboard

This dashboard has 3 major sections and it shows the highest aggregation level - tenant level, or tenant level with namespace level separation. 
Variabiable section, from which you can choose 
A Cluster, and
A Tenant
Overview section shows the aggregated total metrics values summarized at the tenant level. In particular, the following metrics are included
Total number of namespaces
Total number of topics
Total number of producers
Total number of consumers
Total number of subscriptions
Total message storage size (logical) - before replication
Total message storage size -  after replication
Total message size offloaded to a tiered storage
Total message backlog
Total message replication backlog
Total hourly incoming message number
Total hourly incoming message average size
Messaging section shows the time series metrics chart summarized at the tenant level. In particular, the following metrics are included:
Total incoming message rate (msg/s) of the tenant divided by namespaces
Total outgoing message rate (msg/s) of the tenant divided by namespaces
Total incoming message throughput (byte/s) of the tenant divided by namespaces
Total outgoing message throughput (byte/s) of the tenant divided by namespaces
Total message backlog of the tenant divided by namespaces
Total message storage size of the tenant divided by namespaces 
Total Message replication backlog rate (msg/s) divided of the tenant divided by remote clusters
Total Producer/Consumer/Subscription count of the tenant
Total unacknowledged messages of the tenant divided by namespaces
Total message drop rate of the tenant divided by namespaces
Total incoming message replication rate (msg/s) of the tenant divided by remote clusters
Total outgoing message replication rate (msg/s) of the tenant divided by remote clusters
Total incoming message replication throughput (byte/s) of the tenant divided by remote clusters
Total outgoing message replication throughput (byte/s) of the tenant divided by remote clusters
Top 10 topics of the tenant by message backlog
Top 10 topics of the tenant by message replication backlog
Top 10 topics of the tenant by unacknowledged message
Top 10 topics of the tenant by message storage size

== Namespace Dashboard
This dashboard allows zooming in the metrics analysis at the namespace level. The structure of the dashboard is similar to that of the Overview dashboard and includes the following sections.
Variabiable section, from which you can choose 
A Cluster, and
A Tenant, and
A namespace
Overview section shows the aggregated total metrics values summarized at the namespace level.
Total number of topics
Total number of producers
Total number of consumers
Total number of subscriptions
Total message backlog
Total message replication backlog
Total message storage size
Total message size offloaded to a tiered storage
Total hourly incoming message number
Total hourly incoming message average size
Messaging section shows the time series metrics chart summarized at the namespace level with one more level of detail (by topics). In particular, the following metrics are included:
Total incoming message rate (msg/s) of the namespace divided by topics.
Total outgoing message rate (msg/s) of the namespace divided by topics.
Total incoming message throughput (byte/s) of the namespace divided by topics
Total outgoing message throughput (byte/s) of the namespace divided by topics
Total message backlog of the namespace divided by topics
Total unacknowledged messages of the namespace divided by topics
Total message drop rate of the namespace divided by topics
Total Producer/Consumer/Subscription count of the namespace divided by topics
Geo-replication section shows the time series metrics chart summarized at the namespace level with one more level of detail (by remote clusters). In particular, the following metrics are included:
Total incoming replication rate (msg/s) to the namespace divided by remote clusters
Total outgoing replication rate (msg/s) from the namespace divided by remote clusters
Total incoming replication throughput (byte/s) to the namespace divided by remote clusters
Total outgoing replication throughput (byte/s) from the namespace divided by remote clusters
Total (outgoing) message replication backlog from the namespace divided by remote clusters


This dashboard further zooms in the metrics analysis to the (parent) topic level (not at the partition level). The structure of the dashboard is similar to the previous dashboards and includes the following sections.
Variabiable section, from which you can choose 
A Cluster, and
A Tenant, and
A namespace, and
A topic
Overview section shows the aggregated total metrics values summarized at the topic level.
Total number of producers
Total number of consumers
Total number of subscriptions
Total message backlog
Total message replication backlog
Total message storage size -  after replication
Total message size offloaded to a tiered storage
Total hourly incoming message number
Total hourly incoming message average size
Messaging section shows the time series metrics chart summarized at the topic level with one more level of detail (by partitions). In particular, the following metrics are included:
Total incoming message rate (msg/s) of the topic divided by partitions
Total outgoing message rate (msg/s) of the topic divided by partitions
Total incoming message throughput (bytes/s) of the topic divided by partitions
Total outgoing message throughput (bytes/s) of the topic divided by partitions
Total message backlog of the topic divided by partitions
Total unacknowledged messages of the topic divided by partitions
Subscription section shows the time series metrics chart summarized at the subscription level with one more level of detail (by individual subscriptions). In particular, the following metrics are included:
Total subscription message backlog divided by individual subscriptions
Total subscription message backlog with no delay divided by individual subscriptions
Total subscription unacknowledged messages divided by individual subscriptions
Total subscription delayed messages divided by individual subscriptions
Total subscription message dispatch rate (msg/s) divided by individual subscriptions
Total subscription message throughput rate (byte/s) divided by individual subscriptions
Total subscription message acknowledgement rate (msg/s) divided by individual subscriptions
Total subscription message redelivery rate (msg/s) divided by individual subscriptions
Total subscription message expired rate (msg/s) divided by individual subscriptions
Total subscription message dropped rate (msg/s) divided by individual subscriptions
Total subscription messages processed by EntryFilter, divided by individual subscriptions
Total subscription messages accepted by EntryFilter, divided by individual subscriptions
Total subscription messages rejected by EntryFilter, divided by individual subscriptions
Total subscription messages rescheduled by EntryFilter, divided by individual subscriptions
Geo-replication section shows the time series metrics chart summarized at the topic level with one more level of detail (by remote clusters). In particular, the following metrics are included:
Incoming replication rate (msg/s) to the topic divided by remote clusters
Outgoing replication rate (msg/s) from the topic divided by remote clusters
Incoming replication throughput (byte/s) to the topic divided by remote clusters
Outgoing replication throughput (byte/s) from the topic divided by remote clusters
Total (outgoing) message replication backlog from the topic divided by remote clusters

Appendix A: Configure Astra Streaming Metrics Integration with External Prometheus + Grafans Servers 
Overview
Astra Streaming exposes some of the Pulsar metrics through the Prometheus endpoints. From the Astra streaming UI, you can get the prometheus configuration information (as below) to scrape the Astra streaming metrics into an external Prometheus server. In this chapter, we’ll show you how to set up a Prometheus server in a K8s cluster and then configure it properly to scrape the Astra streaming metrics.


Procedures
Prerequisite: Set up a K8s Cluster. 

In our example, we’re using the Prometheus Community Kubernetes Helm Charts to set up a Prometheus server and a Grafana server in a K8s cluster.

Create a configuration yaml file (e.g. astra-msgenrich.yml) using the Astra Streaming Prometheus configuration from the UI.
- job_name: 'astra-pulsar-metrics-msgenrich'
   scheme: 'https'
   metrics_path: '/pulsarmetrics/msgenrich'
   authorization:
      credentials: <jwt_token_value>
   static_configs:
   - targets: ['prometheus-gcp-uscentral1.streaming.datastax.com']

Please NOTE that the above configuration, DO NOT add other prometheus configurations like scrape_interval or evaluation_interval. They will be added in later steps when configuring the helm chart.

Make a K8s secret, named astra-msgenrich, out of the above configuration file.
==Create a K8s secret for additional Prometheus scrape config
kubectl create secret generic astra-msgenrich \
  --from-file=astra-msgenrich.yml \
  --dry-run=client -oyaml > k8s-additional-scrape-config-msgenrich.yaml

kubectl apply -f k8s-additional-scrape-config-msgenrich.yaml


Create a customized values file (e.g. custom-values.yaml) for Prometheus Community Kubernetes helm chart. Please pay attention to the additionalScrapeConfigsSecret section and make sure that the name and key are matching the K8s secret name and file name in the previous 2 steps.
prometheus:
   prometheusSpec:
      scrapeInterval: 60s
      evaluationInterval: 15s
      additionalScrapeConfigsSecret:
         enabled: true
         name: astra-msgenrich
         key: astra-msgenrich.yml


Deploy the helm chart
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

helm upgrade --install kubeprom -f custom-values.yaml prometheus-community/kube-prometheus-stack


(Optional) For Mac users (e.g. using docker-desktop as the underlying K8s cluster), run the following commands to make sure the deployment is successful. Otherwise, there might be issues to bring up the Prometheus node exporter daemon set.
== Patch the Prometheus node exporter daemon set
kubectl patch ds kubeprom-prometheus-node-exporter --type "json" -p '[{"op": "remove", "path" : "/spec/template/spec/containers/0/volumeMounts/2/mountPropagation"}]'

== Confirm if the Prometheus node exporter daemonset is properly up and running.
kubectl rollout status daemonset \
  kubeprom-prometheus-node-exporter \
  --timeout 60s


Verify the Integration
At this point, the Astra Streaming metrics integration with an external Prometheus server should be ready. We can double check this by going to the external Prometheus server UI. 


Please NOTE that the status of the additional scrape job, astra-pulsar-metrics-msgenrich, should be in the UP state. Otherwise, there are some issues in the previous configuration procedures. 
401 Unauthorized Issue
One common issue that we see from integrating the Astra Streaming metrics into an external Prometheus server is the state of the additional scrap job shows as 401 Unauthorized. This is most likely because the JWT token we used in the previous step 1 has expired. 

In order to fix this issue, go to the Astra Streaming UI and create a new JWT token (preferably with no expiration date). Get the new token value and repeat the above procedure.
Appendix B: Configure Astra Streaming Metrics Integration with New Relic
NOTE: the method described in this section also applies to integrating Astra Streaming metrics with other cloud observability platforms like Grafana Cloud.
Overview
According to the New Relic document, currently there are 3 different ways to integrate external Prometheus data into it:
Option 1: Prometheus Agent for Kubernetes.
Option 2: Prometheus OpenMetrics integration for Docker.
Option 3: Prometheus remote write integration
For Astra streaming, options 1 and 2 are NOT relevant; and option 3 is the only possible option. In theory, in order for option 3 to work for Astra Streaming, it requires modifying the configuration of the Prometheus server which Astra Streaming is relying on and so allows Astra Streaming to be able to send the metrics to New Relic directly.  

This, however, is not possible due to the nature of Astra Streaming being a managed service. If an extra Prometheus server can be installed (e.g. as per the instruction in Appendix A), we can use this extra Prometheus server to act as a bridge to forward the scraped Astra Streaming metrics to New Relic. The diagram below shows this idea:


Procedures
Prerequisite: Set up a New Relic account
NOTE: when creating a New Relic account, it will generate a license key. Please save it to a local file for usage in later steps.

Log in the New Relic UI with your account

Click “Add Data” and then choose “Prometheus Remote Write Integration” under “Open source monitoring” category


In the new window, first give a name of your local Prometheus server (e.g. prometheus-docker-desktop) in the example below. Then click “Generate URL” which will generate the url endpoints that you will need to use to configure “remote_write integration” for your local Prometheus server (this corresponds to step (3) in the screenshot below)


Configure and restart your local Prometheus server. In our example, since the local Prometheus server is installed in a local docker-desktop K8s cluster, the installation and configuration method is K8s oriented. 

Create a K8s secrete that corresponds to the New Relic license key that you get while setting up the account
kubectl create secret generic nr-license-key --from-literal=value=<liencse_key_value>


Modify the Prometheus configuration in the kube-prometheus-stack helm chart file (e.g. custom-values.yaml)

prometheus:
   prometheusSpec:
      scrapeInterval: 60s
      evaluationInterval: 15s
      additionalScrapeConfigsSecret:
      enabled: true
         name: astra-msgenrich
         key: astra-msgenrich.yml

      remoteWrite:
        - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=prometheus-docker-desktop
           authorization:
              credentials:
                 key: value
        	   name: nr-license-key


Please NOTE that the top part of the configuration is for scraping Astra Streaming metrics (as we explored in Appendix A) and the bottom part of the configuration is for sending local Prometheus to New Relic via remote_write.  The remote_write url and authorization settings are from the previous steps (step 3 and step 4.1 in particular).

Restart your local Prometheus server

Go back to New Relic UI. If there is no error in the previous step, you should be able to view Astra Streaming metrics in New Relic UI. Below is a screenshot from New Relic data browsing UI in which we can see the Pulsar message backlog metrics are shown on the UI and aggregated by different Pulsar namespaces.

